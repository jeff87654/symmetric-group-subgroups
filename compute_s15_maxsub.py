#!/usr/bin/env python3
"""
compute_s15_maxsub.py - Direct workers for S15 computation (Phase A-3)

Runs ONLY the direct workers that don't need recursive decomposition:
  - intrans_1x14: Embed S14 cache into S15 (uses cached 75,154 reps)
  - wreath_3wr5, wreath_5wr3: Small enough for direct ConjugacyClassesSubgroups
  - primitive_1..4: Very small groups

Intransitive workers (intrans_2x13 through intrans_7x8) are handled by:
  - phase_a1_enumerate.py (recursive tree enumeration)
  - phase_a2_compute_leaves.py (parallel leaf computation)

Memory allocation (64GB machine):
  Phase 1 (solo): intrans_1x14 (32g, loads S14 cache)
  Phase 2 (parallel): wreath_3wr5 + wreath_5wr3 (16g each)
  Phase 3 (all parallel): primitives (4g each)
"""

import subprocess
import sys
import os
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed

# ============================================================================
# Configuration
# ============================================================================

GAP_BASH = r"C:\Program Files\GAP-4.15.1\runtime\bin\bash.exe"
BASE_DIR = Path(r"C:\Users\jeffr\Downloads\Symmetric Groups")
OUTPUT_DIR = BASE_DIR / "maxsub_output_s15"
LOG_DIR = BASE_DIR / "maxsub_output_s15"
CACHE_DIR = BASE_DIR / "conjugacy_cache"

N = 15
EXPECTED_COUNT = 159129

# Timeout per worker
DEFAULT_TIMEOUT = 12 * 3600   # 12 hours
LARGE_TIMEOUT = 48 * 3600     # 48 hours for very large maxsubs

def windows_to_cygwin_path(win_path: str) -> str:
    """Convert Windows path to Cygwin path."""
    path = str(win_path).replace('\\', '/')
    if len(path) >= 2 and path[1] == ':':
        drive = path[0].lower()
        path = f'/cygdrive/{drive}{path[2:]}'
    return path

BASE_CYGWIN = windows_to_cygwin_path(str(BASE_DIR))

# ============================================================================
# Worker definitions (direct workers only)
#
# Phase 1: intrans_1x14 solo (loads S14 cache)
# Phase 2: wreath products (parallel)
# Phase 3: primitives (all parallel, small)
# ============================================================================

# Direct workers only - intransitive workers are handled by the recursive
# decomposition pipeline (phase_a1_enumerate.py + phase_a2_compute_leaves.py)
WORKERS = [
    # Phase 1: intrans_1x14 solo (loads S14 cache, 32g for 75k groups)
    {
        "label": "intrans_1x14",
        "type": "s1xs14",
        "params": [],
        "memory": "32g",
        "timeout": LARGE_TIMEOUT,
        "phase": 1,
    },

    # Phase 2: Wreath products (can run in parallel)
    {
        "label": "wreath_3wr5",
        "type": "wreath",
        "params": [3, 5],
        "memory": "16g",
        "timeout": DEFAULT_TIMEOUT,
        "phase": 2,
    },
    {
        "label": "wreath_5wr3",
        "type": "wreath",
        "params": [5, 3],
        "memory": "16g",
        "timeout": DEFAULT_TIMEOUT,
        "phase": 2,
    },

    # Phase 3: All primitive groups (small, all parallel)
    # Primitives added dynamically in Phase 3
]


def generate_worker_script(worker: dict) -> str:
    """Generate a self-contained GAP script for a single worker."""

    label = worker["label"]
    wtype = worker["type"]
    params = worker["params"]
    output_cygwin = windows_to_cygwin_path(str(OUTPUT_DIR / f"{label}.g"))

    # Common preamble: load core functions
    preamble = f'''# Worker script for maximal subgroup: {label}
# Generated by compute_s15_maxsub.py at {datetime.now()}

MAXSUB_BASE := "{BASE_CYGWIN}";
MAXSUB_OUTPUT := Concatenation(MAXSUB_BASE, "/maxsub_output_s15");
MAXSUB_CACHE := Concatenation(MAXSUB_BASE, "/conjugacy_cache");

Read(Concatenation(MAXSUB_BASE, "/compute_s15_maxsub.g"));

Print("Worker started: type={wtype}, label={label}\\n");
startTime := Runtime();
n := {N};
'''

    if wtype == "s1xs14":
        body = f'''
# Special case: use cached S14 subgroups
Print("Loading S14 cache and embedding as S1 x S14...\\n");
cacheFile := Concatenation(MAXSUB_CACHE, "/s14_subgroups.g");
embeddedSubs := EmbedPreviousSnSubgroups(cacheFile, n, 14, "intrans_1x14");

# Write directly to output
PrintTo("{output_cygwin}", "# Subgroups of S1 x S14 (from S14 cache)\\n");
AppendTo("{output_cygwin}", "# Count: ", Length(embeddedSubs), "\\n");
AppendTo("{output_cygwin}", "maxsub_results := [\\n");

for i in [1..Length(embeddedSubs)] do
    entry := embeddedSubs[i];
    gens := GeneratorsOfGroup(entry.group);
    genImages := [];
    for g in gens do
        Add(genImages, ListPerm(g, n));
    od;

    if i > 1 then
        AppendTo("{output_cygwin}", ",\\n");
    fi;
    AppendTo("{output_cygwin}", "  rec(gens := ", genImages,
             ", inv := ", entry.inv,
             ", source := \\"intrans_1x14\\")");

    if i mod 2000 = 0 then
        Print("  Written ", i, "/", Length(embeddedSubs), "\\n");
        GASMAN("collect");
    fi;
od;

AppendTo("{output_cygwin}", "\\n];\\n");

elapsed := Runtime() - startTime;
Print("\\n=== S1 x S14 worker complete ===\\n");
Print("  Subgroups: ", Length(embeddedSubs), "\\n");
Print("  Time: ", Int(elapsed/1000), " seconds\\n");
AppendTo("{output_cygwin}", "# Complete: ", Length(embeddedSubs),
         " subgroups in ", Int(elapsed/1000), " seconds\\n");
'''

    elif wtype == "intransitive":
        k = params[0]
        body = f'''
k := {k};
Print("Building S_", k, " x S_", n-k, " on {{1,...,", n, "}}...\\n");
M := BuildIntransitiveMaxSub(k, n);
workerLabel := "intrans_{k}x{N - k}";

Print("Maximal subgroup: ", workerLabel, "\\n");
Print("  Order: ", Size(M), "\\n");

count := ComputeSubgroupsOfMaxSub(M, workerLabel, "{output_cygwin}", n);

elapsed := Runtime() - startTime;
AppendTo("{output_cygwin}", "# Complete: ", count,
         " subgroups in ", Int(elapsed/1000), " seconds\\n");

Print("\\n=== Worker ", workerLabel, " complete ===\\n");
Print("  Subgroups: ", count, "\\n");
Print("  Total time: ", Int(elapsed/1000), " seconds\\n");
'''

    elif wtype == "wreath":
        a, b = params
        body = f'''
Print("Building S_{a} wr S_{b}...\\n");
M := BuildWreathMaxSub({a}, {b});
workerLabel := "wreath_{a}wr{b}";

Print("Maximal subgroup: ", workerLabel, "\\n");
Print("  Order: ", Size(M), "\\n");

count := ComputeSubgroupsOfMaxSub(M, workerLabel, "{output_cygwin}", n);

elapsed := Runtime() - startTime;
AppendTo("{output_cygwin}", "# Complete: ", count,
         " subgroups in ", Int(elapsed/1000), " seconds\\n");

Print("\\n=== Worker ", workerLabel, " complete ===\\n");
Print("  Subgroups: ", count, "\\n");
Print("  Total time: ", Int(elapsed/1000), " seconds\\n");
'''

    elif wtype == "primitive":
        idx = params[0]
        body = f'''
Print("Loading PrimitiveGroup({N}, {idx})...\\n");
M := PrimitiveGroup({N}, {idx});
workerLabel := "primitive_{idx}";

Print("Maximal subgroup: ", workerLabel, "\\n");
Print("  Order: ", Size(M), "\\n");

count := ComputeSubgroupsOfMaxSub(M, workerLabel, "{output_cygwin}", n);

elapsed := Runtime() - startTime;
AppendTo("{output_cygwin}", "# Complete: ", count,
         " subgroups in ", Int(elapsed/1000), " seconds\\n");

Print("\\n=== Worker ", workerLabel, " complete ===\\n");
Print("  Subgroups: ", count, "\\n");
Print("  Total time: ", Int(elapsed/1000), " seconds\\n");
'''

    else:
        body = f'''
Print("ERROR: Unknown worker type: {wtype}\\n");
'''

    return preamble + body + "\nQUIT;\n"


def generate_primitive_workers() -> list:
    """Generate worker definitions for primitive maximal subgroups of S15.

    Primitive groups of degree 15 (from pre-flight):
      PrimitiveGroup(15, 1) = A7, order 2520
      PrimitiveGroup(15, 2) = A6, order 360
      PrimitiveGroup(15, 3) = S6, order 720
      PrimitiveGroup(15, 4) = A8, order 20160
      PrimitiveGroup(15, 5) = A15 (skip)
      PrimitiveGroup(15, 6) = S15 (skip)
    """
    print("Querying GAP for primitive groups of degree 15...")

    query_script = f'''
nrPrim := NrPrimitiveGroups({N});
Print("NRPRIM=", nrPrim, "\\n");
for i in [1..nrPrim] do
    G := PrimitiveGroup({N}, i);
    ord := Size(G);
    # Skip S_n and A_n
    if ord < Factorial({N}) and ord < Factorial({N})/2 then
        Print("PRIM=", i, ",", ord, "\\n");
    fi;
od;
QUIT;
'''
    script_file = OUTPUT_DIR / "query_primitives.g"
    script_cygwin = windows_to_cygwin_path(str(script_file))
    with open(script_file, 'w') as f:
        f.write(query_script)

    cmd = f'/opt/gap-4.15.1/gap -q "{script_cygwin}"'
    proc = subprocess.run(
        [GAP_BASH, '--login', '-c', cmd],
        capture_output=True, text=True, timeout=60
    )

    workers = []
    for line in proc.stdout.strip().split('\n'):
        line = line.strip()
        if line.startswith("PRIM="):
            parts = line[5:].split(',')
            idx = int(parts[0])
            order = int(parts[1])
            workers.append({
                "label": f"primitive_{idx}",
                "type": "primitive",
                "params": [idx],
                "memory": "4g",
                "timeout": DEFAULT_TIMEOUT,
                "phase": 3,
            })
            print(f"  PrimitiveGroup(15, {idx}): order {order}")

    print(f"  Found {len(workers)} non-trivial primitive maximal subgroups\n")
    return workers


def run_worker(worker: dict) -> dict:
    """Run a single GAP worker process."""
    label = worker["label"]
    memory = worker["memory"]
    timeout = worker["timeout"]

    # Generate and write the GAP script
    script = generate_worker_script(worker)
    script_file = OUTPUT_DIR / f"worker_{label}.g"
    with open(script_file, 'w') as f:
        f.write(script)

    script_cygwin = windows_to_cygwin_path(str(script_file))
    log_file = LOG_DIR / f"worker_{label}.log"

    cmd = f'/opt/gap-4.15.1/gap -q -o {memory} "{script_cygwin}"'

    start_time = time.time()
    result = {
        "label": label,
        "success": False,
        "elapsed": 0,
        "returncode": -1,
        "error": None,
    }

    try:
        with open(log_file, 'w') as log:
            log.write(f"# Worker: {label}\n")
            log.write(f"# Started: {datetime.now()}\n")
            log.write(f"# Memory: {memory}\n")
            log.write(f"# Command: {cmd}\n\n")
            log.flush()

            proc = subprocess.Popen(
                [GAP_BASH, '--login', '-c', cmd],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
            )

            for line in proc.stdout:
                log.write(line)
                log.flush()
                # Print key progress lines to console
                if any(kw in line for kw in ["Computing", "Found", "Complete",
                                              "ERROR", "Saved", "Processed",
                                              "Embedded", "Written"]):
                    print(f"  [{label}] {line.rstrip()}")

            proc.wait(timeout=timeout)
            result["returncode"] = proc.returncode

    except subprocess.TimeoutExpired:
        proc.kill()
        result["error"] = f"Timeout after {timeout}s"
        print(f"  [{label}] TIMEOUT after {timeout/3600:.1f} hours")
    except Exception as e:
        result["error"] = str(e)
        print(f"  [{label}] ERROR: {e}")

    elapsed = time.time() - start_time
    result["elapsed"] = elapsed

    # Check if output file was created with Complete marker
    output_file = OUTPUT_DIR / f"{label}.g"
    if output_file.exists() and output_file.stat().st_size > 100:
        with open(output_file, 'r') as f:
            content = f.read()
            if "# Complete:" in content:
                result["success"] = True
                print(f"  [{label}] Done in {elapsed:.0f}s (exit code {result['returncode']})")
            else:
                result["success"] = False
                result["error"] = "Output file exists but no Complete marker"
                print(f"  [{label}] INCOMPLETE: no Complete marker in output")
    else:
        result["success"] = False
        if not result["error"]:
            result["error"] = "Output file missing or empty"
        print(f"  [{label}] FAILED: {result['error']}")

    return result


def run_phase(phase_num: int, workers: list, max_parallel: int) -> list:
    """Run all workers in a phase with controlled parallelism."""
    phase_workers = [w for w in workers if w["phase"] == phase_num]
    if not phase_workers:
        return []

    print(f"\n{'='*60}")
    print(f"Phase {phase_num}: {len(phase_workers)} workers (max {max_parallel} parallel)")
    print(f"{'='*60}")

    for w in phase_workers:
        print(f"  - {w['label']} (memory: {w['memory']}, timeout: {w['timeout']//3600}h)")
    print()

    results = []
    with ProcessPoolExecutor(max_workers=max_parallel) as executor:
        futures = {executor.submit(run_worker, w): w for w in phase_workers}
        for future in as_completed(futures):
            worker = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"  [{worker['label']}] Exception: {e}")
                results.append({
                    "label": worker["label"],
                    "success": False,
                    "elapsed": 0,
                    "error": str(e),
                })

    return results


def check_existing_outputs() -> list:
    """Check which worker outputs already exist (for resume support)."""
    existing = []
    for f in OUTPUT_DIR.glob("*.g"):
        if f.name.startswith("worker_") or f.name.startswith("query_") or f.name.startswith("verify_"):
            continue
        if f.stat().st_size > 100:
            # Check if the file has a "Complete" marker
            with open(f, 'r') as fh:
                content = fh.read()
                if "# Complete:" in content:
                    label = f.stem
                    existing.append(label)
    return existing


def main():
    print("=" * 60)
    print("A000638(15) = 159,129 via Maximal Subgroup Decomposition")
    print("=" * 60)
    print(f"Started: {datetime.now()}")
    print(f"Output dir: {OUTPUT_DIR}")
    print()

    # Create output directory
    OUTPUT_DIR.mkdir(exist_ok=True)
    (OUTPUT_DIR / "dedup_work").mkdir(exist_ok=True)
    CACHE_DIR.mkdir(exist_ok=True)

    # Check for existing completed outputs (resume support)
    existing = check_existing_outputs()
    if existing:
        print(f"Found {len(existing)} existing completed outputs:")
        for label in sorted(existing):
            print(f"  - {label}")
        print()

    # Get primitive group workers
    prim_workers = generate_primitive_workers()

    # Build full worker list
    all_workers = WORKERS + prim_workers

    # Skip already-completed workers
    workers_to_run = [w for w in all_workers if w["label"] not in existing]
    skipped = [w for w in all_workers if w["label"] in existing]

    if skipped:
        print(f"Skipping {len(skipped)} already-completed workers:")
        for w in skipped:
            print(f"  - {w['label']}")
        print()

    print(f"Total workers: {len(all_workers)}")
    print(f"Workers to run: {len(workers_to_run)}")
    print()

    # ========================================================================
    # Phase A: Run workers in phases
    # ========================================================================

    all_results = []
    overall_start = time.time()

    if workers_to_run:
        # Phase 1: intrans_1x14 solo (32g, loads S14 cache)
        results = run_phase(1, workers_to_run, max_parallel=1)
        all_results.extend(results)

        # Phase 2: wreath products (16g each, can run in parallel)
        results = run_phase(2, workers_to_run, max_parallel=2)
        all_results.extend(results)

        # Phase 3: all primitive groups (small, all parallel)
        results = run_phase(3, workers_to_run, max_parallel=8)
        all_results.extend(results)

    # Summary of Phase A
    phase_a_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("Phase A Summary")
    print(f"{'='*60}")

    succeeded = [r for r in all_results if r["success"]]
    failed = [r for r in all_results if not r["success"]]

    print(f"  Completed: {len(succeeded)}/{len(all_results)}")
    print(f"  Previously completed: {len(skipped)}")
    print(f"  Failed: {len(failed)}")
    print(f"  Phase A time: {phase_a_elapsed:.0f}s ({phase_a_elapsed/3600:.1f}h)")

    if failed:
        print("\n  Failed workers:")
        for r in failed:
            print(f"    - {r['label']}: {r.get('error', 'unknown error')}")

        print("\n  WARNING: Some workers failed. Fix and re-run to retry failed workers.")
        print("  (Successfully completed workers will be skipped on re-run.)")
        return 1

    total_elapsed = time.time() - overall_start
    print(f"\n{'='*60}")
    print("Phase A Complete!")
    print(f"{'='*60}")
    print(f"  All workers succeeded.")
    print(f"  Total time: {total_elapsed:.0f}s ({total_elapsed/3600:.1f}h)")
    print(f"  Output dir: {OUTPUT_DIR}")
    print(f"  Finished: {datetime.now()}")
    print(f"\n  Next step: Run phase_a4_combine.py to combine all results,")
    print(f"  then phase_b1_s15.py for bucketing, then rerun_phase_b2b3_s15.py for dedup.")

    return 0


if __name__ == "__main__":
    sys.exit(main())
